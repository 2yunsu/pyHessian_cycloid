## Publication List

Below is a list of related work that have been written on or with the help of PyHessian.


1. [ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning](https://arxiv.org/pdf/2006.00719.pdf)
<details>
      <summary>
      Summary
      </summary>  
      * AdaHessian is a new second order optimizer that uses Hessian diagonal to adaptively adjust gradient
      * The key idea is a novel inexact Newton method with variance reduction (RMS in time along with spatial averaging)
      * We perform extensive tests with modern Neural Networks for CV, NLP, and recommendation systems, and we achieve the best performance for all tasks as compared to other optimizers, even though they are tuned at industrial scale.
      * This is the first time that a second order method can exceed ADAM/SGD performance (and in fact with a large margin)
</details>
1. [G-DAUG: Generative Data Augmentation for Commonsense Reasoning](https://arxiv.org/pdf/2004.11546.pdf)
1. [Constraint-Based Regularization of Neural Networks](https://arxiv.org/pdf/2006.10114.pdf)
1. [Batch Normalization Provably Avoids Rank Collapse for Randomly Initialised Deep Networks](https://arxiv.org/pdf/2003.01652.pdf)
1. [Lipschitz Recurrent Neural Networks](https://arxiv.org/pdf/2006.12070.pdf)
